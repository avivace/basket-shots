\section{Esperimenti e analisi dei risultati}

\begin{table}[h!]
\centering
  \begin{tabular}{l l} 
  Accuracy complessiva & xx\\
  Precision per la classe \textit{made} & xx\\
  Precision per la classe \textit{missed} & xx\\
  Recall per la classe \textit{made} & xx\\
  Recall per la classe \textit{missed} & xx\\
  F-measure per la classe \textit{made} & xx\\
  F-measure per la classe \textit{missed} & xx\\
  Area Under Curve per la classe \textit{made} & xx\\
  Area Under Curve per la classe \textit{missed} & xx\\
  Area Under Curve complessiva & xx\\
    \end{tabular}
    \caption{Metriche risultate dell'esecuzione della cross validation su SVM}
\end{table}


Queste metriche sono state calcolate direttamente dalla \autoref{confusion_matrix_svm}.

\begin{table}

\centering
\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
\centering
  \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering Actual value}}} & 
    & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
  & & \bfseries p & \bfseries n & \bfseries total \\
  & p$'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & P$'$ \\[2.4em]
  & n$'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & N$'$ \\
  & total & P & N &
\end{tabular}
 \caption{Confusion Matrix}
 \label{confusion_matrix_svm}
\end{table}

\par
Il risultato, intuitivamente un po' basso, è in realtà in linea con le aspettative: come già anticipato nella sezione 3.1, le circostanze che influenzano un tiro a canestro sono molteplici, mentre le informazioni in nostro possesso sono limitate. Il fatto che sia comunque  superiore al random guessing dimostra che esistono dei fattori nel dataset che tendono effettivamente ad influenzare il successo o il fallimento del tiro.

\par
Dalle metriche ottenute possiamo fare alcune valutazioni: dai valori di precision molto vicini tra loro deduciamo che il numero di falsi positivi per entrambe le classi è simile ed inferiore al 60\%.
% TO DO: false positive per made e false negative per missed?
Guardando le recall invece notiamo che per la classe \textit{missed} tendono ad esserci pochi falsi negativi, mentre la vera criticità è il numero molto alto di falsi negativi per \textit{made}. In altre parole, un tiro legittimo tende ad essere considerato un fallimento.

Le F-measure sono valori che permettono di confrontare direttamente le due classi. Precision e recall sono spesso diverse e quindi è complicato determinare quale classe abbia effettivamente meno errori: la F-measure, ottenuta computando la media armonica di queste due metriche, permette di quantificare le performance delle due classi in maniera comparabile. Nel nostro caso era deducibile che la f-measure di “missed” fosse molto più alta di quella di “made”: la disparità nelle recall influisce molto sul calcolo complessivo.

Essendo un problema di apprendimento automatico binario, la Area Under Curve, intesa come AUROC (Area Under Receiver Operating Characteristic), è solo una. Tale metrica corrisponde all’area sottostante la ROC, ottenuta muovendo gradualmente la threshold della SVM e rappresentando sul grafo ogni volta i valori di TPR (True Positive Rate) e FPR (False Positive Rate).
Più grande (vicino a 1) è il valore della AUROC, più è grande la distinzione tra True Positive e True Negative: nel nostro caso vale 0,64.

\par
Sia eseguendo SVM sull'intero dataset, diviso opportunamente in training set (80\%) e trainset (il restante 20\%), sia eseguendolo con una 10-fold cross validation, il valore di accuracy complessivo tra le due esecuzioni non si discosta troppo. Ciò porta a concludere che il dataset è formato da istanze poco sofferenti di bias e difficilmente rischia di andare in overfitting.

\par
Non essendo completamente soddisfatti dal livello di accuracy ottenuto, abbiamo cercato lavori analoghi al nostro per confrontare e verificare la possibile presenza di errori nella realizzazione del modello. \cite{predictingNBAst} fa un percorso analogo al nostro partendo dallo stesso datasets \textit{Shot logs}, confrontando più algoritmi e raggiungendo un'accuratezza  68\% utilizzando \textit{XGBoost}, un'implementazione di \textit{Boosting}. È un approccio che coinvolge diversi classificatori \textit{weak learners} e infine, ne combina le predizioni, pesate differentemente, con un ulteriore algoritmo di apprendimento artificiale (\textit{strong learner}).

\subsection{Alberi di Decisione}
Per verificare come si comportassero altri modelli diversi dalla SVM, abbiamo dato in input il nostro dataset e abbiamo allenato un modello sfruttando la tecnica del Decision Tree.
Innanzitutto è stata eseguita un’euristica per individuare il valore migliore di CP, ossia quello con “xerror” minimo. Questo parametro indica di quanto debba migliorare il relative error complessivo per poter eseguire uno split nel nodo dell'albero.
Il risultato però è stato un albero con un solo split:

\begin{figure}
\caption{Alberi di decisioni}
\label{dt_fig}
  \includegraphics[width=\linewidth]{DECISIONTREE}
\end{figure}


Un’analisi delle metriche ci mostra il perchè di un albero così corto:

\begin{table}[h!]
\centering
  \begin{tabular}{l l} 
  Accuracy complessiva & 61.06\\
  Precision per la classe \textit{made} & 62.12\\
  Precision per la classe \textit{missed} & 60.68\\
  Recall per la classe \textit{made} & 35.71\\
  Recall per la classe \textit{missed} & 82.01\\
  F-measure per la classe \textit{made} & 45.35\\
  F-measure per la classe \textit{missed} & 69.75\\
    \end{tabular}
    \caption{Metriche risultate dell'esecuzione della cross validation su Decision Tree}
\end{table}

Queste metriche sono state calcolate direttamente dalla \autoref{confusion_matrix_dt}.


\begin{table}

\centering
\noindent
\renewcommand\arraystretch{1.5}
\setlength\tabcolsep{0pt}
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
\centering
  \multirow{10}{*}{\rotatebox{90}{\parbox{1.1cm}{\bfseries\centering Actual value}}} & 
    & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
  & & \bfseries p & \bfseries n & \bfseries total \\
  & p$'$ & \MyBox{20 639}{} & \MyBox{37 162}{} & P$'$ \\[2.4em]
  & n$'$ & \MyBox{12 587}{} & \MyBox{57 357}{} & N$'$ \\
  & total & P & N &
\end{tabular}
 \caption{Confusion Matrix di DT}
 \label{confusion_matrix_dt}
\end{table}



\begin{table}[h!]
\centering
  \begin{tabular}{l l} 
shot\_dist &77\\
shot\_clock &8\\
touch\_time &8\\
close\_def\_distance &7\\
percentage\_prev\_games &1\\
    \end{tabular}
    \caption{Importance in DT}
\end{table}

L’importance di shot\_dist è nettamente più alta di qualsiasi altro attributo.
Questi valori dimostrano che il decision tree non è adeguato per il nostro problema in quanto non coinvolge attributi che per noi sono influenti e probabilmente generalizzerà male con istanze nuove.
Inoltre questo albero di decisione ottiene delle metriche peggiori della nostra SVM.
